{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "449fff0c",
      "metadata": {
        "id": "449fff0c"
      },
      "source": [
        "# Laboratorio 9\n",
        "\n",
        "andre marroquin 22266\n",
        "sergio orellana 221122\n",
        "nelson garcia 22434\n",
        "joaquin puente 22296"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf6mhpkel2k",
      "metadata": {
        "id": "cf6mhpkel2k"
      },
      "source": [
        "## Arquitectura AlexNet para CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "### Imports y configuración inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7383dfdc",
      "metadata": {
        "id": "7383dfdc",
        "outputId": "06f32b02-4bf2-4953-c33e-c60e84fc1adb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# imports principales\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# asegurar reproducibilidad básica\n",
        "def seed_everything(seed: int = 42) -> None:\n",
        "    import random\n",
        "    import numpy as np\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "# elegir dispositivo\n",
        "def get_device() -> torch.device:\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "device = get_device()\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6jy7edbloq",
      "metadata": {
        "id": "b6jy7edbloq"
      },
      "source": [
        "### Funciones de entrenamiento y evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2i5hv9ukmky",
      "metadata": {
        "id": "2i5hv9ukmky"
      },
      "outputs": [],
      "source": [
        "# calcular accuracy\n",
        "def accuracy_from_logits(logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
        "    preds = logits.argmax(dim=1)\n",
        "    correct = (preds == targets).sum().item()\n",
        "    total = targets.numel()\n",
        "    return correct / total\n",
        "\n",
        "# construir matriz de confusión\n",
        "def confusion_matrix(num_classes: int, preds: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "    cm = torch.zeros((num_classes, num_classes), dtype=torch.long)\n",
        "    for t, p in zip(targets.view(-1), preds.view(-1)):\n",
        "        cm[t.long(), p.long()] += 1\n",
        "    return cm\n",
        "\n",
        "# calcular macro-f1 desde matriz de confusión\n",
        "def macro_f1_from_confusion(cm: torch.Tensor) -> float:\n",
        "    cm = cm.to(torch.float32)\n",
        "    tp = torch.diag(cm)\n",
        "    fp = cm.sum(dim=0) - tp\n",
        "    fn = cm.sum(dim=1) - tp\n",
        "\n",
        "    precision = tp / torch.clamp(tp + fp, min=1.0)\n",
        "    recall = tp / torch.clamp(tp + fn, min=1.0)\n",
        "    f1 = 2.0 * precision * recall / torch.clamp(precision + recall, min=1e-12)\n",
        "    return f1.mean().item()\n",
        "\n",
        "# ciclo de entrenamiento por época\n",
        "def train_one_epoch(model: nn.Module,\n",
        "                    loader: DataLoader,\n",
        "                    criterion: nn.Module,\n",
        "                    optimizer: optim.Optimizer,\n",
        "                    device: torch.device,\n",
        "                    log_every: int = 100) -> Dict[str, float]:\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    count = 0\n",
        "\n",
        "    t0 = time.time()\n",
        "    for step, (x, y) in enumerate(loader, 1):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_acc = accuracy_from_logits(logits, y)\n",
        "        running_loss += loss.item()\n",
        "        running_acc += batch_acc\n",
        "        count += 1\n",
        "\n",
        "        if step % log_every == 0:\n",
        "            print(f\"step {step:04d} | loss {running_loss / count:.4f} | acc {running_acc / count:.4f}\")\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    return {\n",
        "        \"train_loss\": running_loss / max(count, 1),\n",
        "        \"train_acc\":  running_acc / max(count, 1),\n",
        "        \"train_time_s\": dt\n",
        "    }\n",
        "\n",
        "# evaluación completa\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module,\n",
        "             loader: DataLoader,\n",
        "             criterion: nn.Module,\n",
        "             device: torch.device,\n",
        "             num_classes: int) -> Dict[str, float]:\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "    count = 0\n",
        "    cm = torch.zeros((num_classes, num_classes), dtype=torch.long)\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += accuracy_from_logits(logits, y)\n",
        "        count += 1\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        cm += confusion_matrix(num_classes, preds.cpu(), y.cpu())\n",
        "\n",
        "    macro_f1 = macro_f1_from_confusion(cm)\n",
        "    return {\n",
        "        \"val_loss\": total_loss / max(count, 1),\n",
        "        \"val_acc\":  total_acc / max(count, 1),\n",
        "        \"val_macro_f1\": macro_f1\n",
        "    }\n",
        "\n",
        "# bucle de entrenamiento de varias épocas con mejor modelo por accuracy\n",
        "def fit(model: nn.Module,\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader,\n",
        "        criterion: nn.Module,\n",
        "        optimizer: optim.Optimizer,\n",
        "        scheduler,\n",
        "        device: torch.device,\n",
        "        num_classes: int,\n",
        "        epochs: int = 5,\n",
        "        name: str = \"model\") -> Tuple[nn.Module, Dict[str, float]]:\n",
        "    best_state = None\n",
        "    best_acc = -1.0\n",
        "    history = {}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f\"\\n==> {name} | epoch {epoch}/{epochs}\")\n",
        "        train_stats = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_stats = evaluate(model, val_loader, criterion, device, num_classes)\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        print(f\"train  | loss {train_stats['train_loss']:.4f} acc {train_stats['train_acc']:.4f} time {train_stats['train_time_s']:.1f}s\")\n",
        "        print(f\"valid  | loss {val_stats['val_loss']:.4f} acc {val_stats['val_acc']:.4f} macro_f1 {val_stats['val_macro_f1']:.4f}\")\n",
        "\n",
        "        # actualizar mejor\n",
        "        if val_stats[\"val_acc\"] > best_acc:\n",
        "            best_acc = val_stats[\"val_acc\"]\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "        history[epoch] = {**train_stats, **val_stats}\n",
        "\n",
        "    # cargar mejor estado antes de devolver\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    print(f\"\\n>> best val acc: {best_acc:.4f}\")\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80oa47ryd3l",
      "metadata": {
        "id": "80oa47ryd3l"
      },
      "source": [
        "### Preparación de datos CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vbvnxd7hpwe",
      "metadata": {
        "id": "vbvnxd7hpwe",
        "outputId": "f533b725-3e92-46c0-98de-3815fc1d36bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 50000, Test samples: 10000\n",
            "Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ]
        }
      ],
      "source": [
        "# transforms para cifar-10\n",
        "# media y desviación estándar de cifar-10\n",
        "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar10_std = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "# data augmentation para entrenamiento\n",
        "train_tf_cifar10 = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
        "])\n",
        "\n",
        "# transforms para validación/test\n",
        "test_tf_cifar10 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
        "])\n",
        "\n",
        "# cargar datasets cifar-10\n",
        "data_root = \"./data\"\n",
        "\n",
        "train_cifar10 = datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_tf_cifar10)\n",
        "test_cifar10 = datasets.CIFAR10(root=data_root, train=False, download=True, transform=test_tf_cifar10)\n",
        "\n",
        "batch_size_cifar10 = 128\n",
        "num_workers = min(4, os.cpu_count() or 0)\n",
        "\n",
        "train_loader_cifar10 = DataLoader(train_cifar10, batch_size=batch_size_cifar10, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "val_loader_cifar10 = DataLoader(test_cifar10, batch_size=batch_size_cifar10, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_cifar10)}, Test samples: {len(test_cifar10)}\")\n",
        "print(f\"Classes: {train_cifar10.classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbd91zkjaes",
      "metadata": {
        "id": "fbd91zkjaes"
      },
      "source": [
        "### Implementación de AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "em933zgllq6",
      "metadata": {
        "id": "em933zgllq6",
        "outputId": "1641ff49-9922-4eaf-f45e-671a83ebc597"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu1): ReLU(inplace=True)\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu2): ReLU(inplace=True)\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu3): ReLU(inplace=True)\n",
              "  (conv4): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu4): ReLU(inplace=True)\n",
              "  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu5): ReLU(inplace=True)\n",
              "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(4, 4))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=4096, out_features=2048, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=2048, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# implementación de AlexNet adaptada para CIFAR-10 (32x32)\n",
        "# la arquitectura original fue diseñada para ImageNet (224x224)\n",
        "# esta versión adapta las dimensiones para imágenes pequeñas\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes: int = 10):\n",
        "        super().__init__()\n",
        "\n",
        "        # capa convolucional 1: entrada 3x32x32 -> salida 64x16x16\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # capa convolucional 2: entrada 64x16x16 -> salida 192x8x8\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # capa convolucional 3: entrada 192x8x8 -> salida 384x8x8\n",
        "        self.conv3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "\n",
        "        # capa convolucional 4: entrada 384x8x8 -> salida 256x8x8\n",
        "        self.conv4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu4 = nn.ReLU(inplace=True)\n",
        "\n",
        "        # capa convolucional 5: entrada 256x8x8 -> salida 256x4x4\n",
        "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu5 = nn.ReLU(inplace=True)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # adaptive pooling para garantizar tamaño fijo\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))\n",
        "\n",
        "        # clasificador (fully connected layers)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(256 * 4 * 4, 2048),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(2048, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # feature extraction\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.relu3(self.conv3(x))\n",
        "        x = self.relu4(self.conv4(x))\n",
        "        x = self.pool5(self.relu5(self.conv5(x)))\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # classification\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# instanciar alexnet\n",
        "alexnet = AlexNet(num_classes=10).to(device)\n",
        "alexnet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tsaspj87nd",
      "metadata": {
        "id": "tsaspj87nd"
      },
      "source": [
        "### Configuración del optimizador y scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "payj0udbh9r",
      "metadata": {
        "id": "payj0udbh9r",
        "outputId": "22a7682c-e2f9-439c-ee7f-fda6ce241a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuración de entrenamiento:\n",
            "Criterio: CrossEntropyLoss\n",
            "Optimizador: SGD (lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
            "Scheduler: StepLR (step_size=10, gamma=0.5)\n"
          ]
        }
      ],
      "source": [
        "# configuración de entrenamiento para alexnet\n",
        "criterion_cifar10 = nn.CrossEntropyLoss()\n",
        "optimizer_cifar10 = optim.SGD(alexnet.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler_cifar10 = optim.lr_scheduler.StepLR(optimizer_cifar10, step_size=10, gamma=0.5)\n",
        "\n",
        "print(\"Configuración de entrenamiento:\")\n",
        "print(f\"Criterio: CrossEntropyLoss\")\n",
        "print(f\"Optimizador: SGD (lr=0.01, momentum=0.9, weight_decay=5e-4)\")\n",
        "print(f\"Scheduler: StepLR (step_size=10, gamma=0.5)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m9hn3b12jk",
      "metadata": {
        "id": "m9hn3b12jk"
      },
      "source": [
        "### Entrenamiento de AlexNet en CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "429p6inne54",
      "metadata": {
        "id": "429p6inne54",
        "outputId": "6ef91349-8255-453e-eaca-6d6a8d059cf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==> alexnet-cifar10 | epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/Japo/Documents/workspaces/uvg/deepLearning/Lab9-DL/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0100 | loss 2.3024 | acc 0.1035\n",
            "step 0200 | loss 2.3013 | acc 0.1082\n",
            "step 0300 | loss 2.2707 | acc 0.1350\n",
            "train  | loss 2.2060 acc 0.1606 time 270.7s\n",
            "valid  | loss 1.8994 acc 0.2955 macro_f1 0.2446\n",
            "\n",
            "==> alexnet-cifar10 | epoch 2/20\n",
            "step 0100 | loss 1.8874 | acc 0.2709\n",
            "step 0200 | loss 1.8454 | acc 0.2919\n",
            "step 0300 | loss 1.8092 | acc 0.3096\n",
            "train  | loss 1.7721 acc 0.3248 time 294.3s\n",
            "valid  | loss 1.5812 acc 0.4044 macro_f1 0.3747\n",
            "\n",
            "==> alexnet-cifar10 | epoch 3/20\n",
            "step 0100 | loss 1.5969 | acc 0.3925\n",
            "step 0200 | loss 1.5710 | acc 0.4049\n",
            "step 0300 | loss 1.5461 | acc 0.4173\n",
            "train  | loss 1.5246 acc 0.4266 time 281.8s\n",
            "valid  | loss 1.3690 acc 0.4812 macro_f1 0.4483\n",
            "\n",
            "==> alexnet-cifar10 | epoch 4/20\n",
            "step 0100 | loss 1.4078 | acc 0.4814\n",
            "step 0200 | loss 1.3772 | acc 0.4886\n",
            "step 0300 | loss 1.3583 | acc 0.4976\n",
            "train  | loss 1.3427 acc 0.5048 time 238.2s\n",
            "valid  | loss 1.1928 acc 0.5558 macro_f1 0.5450\n",
            "\n",
            "==> alexnet-cifar10 | epoch 5/20\n",
            "step 0100 | loss 1.2291 | acc 0.5494\n",
            "step 0200 | loss 1.2159 | acc 0.5552\n",
            "step 0300 | loss 1.2069 | acc 0.5597\n",
            "train  | loss 1.1904 acc 0.5658 time 238.7s\n",
            "valid  | loss 1.0453 acc 0.6263 macro_f1 0.6171\n",
            "\n",
            "==> alexnet-cifar10 | epoch 6/20\n",
            "step 0100 | loss 1.0938 | acc 0.5991\n",
            "step 0200 | loss 1.0687 | acc 0.6130\n",
            "step 0300 | loss 1.0545 | acc 0.6203\n",
            "train  | loss 1.0436 acc 0.6240 time 238.3s\n",
            "valid  | loss 1.0076 acc 0.6398 macro_f1 0.6390\n",
            "\n",
            "==> alexnet-cifar10 | epoch 7/20\n",
            "step 0100 | loss 0.9856 | acc 0.6435\n",
            "step 0200 | loss 0.9801 | acc 0.6449\n",
            "step 0300 | loss 0.9609 | acc 0.6535\n",
            "train  | loss 0.9484 acc 0.6596 time 240.3s\n",
            "valid  | loss 0.9087 acc 0.6803 macro_f1 0.6720\n",
            "\n",
            "==> alexnet-cifar10 | epoch 8/20\n",
            "step 0100 | loss 0.8760 | acc 0.6833\n",
            "step 0200 | loss 0.8629 | acc 0.6887\n",
            "step 0300 | loss 0.8634 | acc 0.6893\n",
            "train  | loss 0.8610 acc 0.6919 time 239.9s\n",
            "valid  | loss 0.7546 acc 0.7352 macro_f1 0.7320\n",
            "\n",
            "==> alexnet-cifar10 | epoch 9/20\n",
            "step 0100 | loss 0.8043 | acc 0.7165\n",
            "step 0200 | loss 0.8017 | acc 0.7157\n",
            "step 0300 | loss 0.7983 | acc 0.7163\n",
            "train  | loss 0.7909 acc 0.7194 time 241.0s\n",
            "valid  | loss 0.7009 acc 0.7541 macro_f1 0.7534\n",
            "\n",
            "==> alexnet-cifar10 | epoch 10/20\n",
            "step 0100 | loss 0.7329 | acc 0.7389\n",
            "step 0200 | loss 0.7392 | acc 0.7386\n",
            "step 0300 | loss 0.7389 | acc 0.7385\n",
            "train  | loss 0.7348 acc 0.7405 time 238.7s\n",
            "valid  | loss 0.7104 acc 0.7511 macro_f1 0.7491\n",
            "\n",
            "==> alexnet-cifar10 | epoch 11/20\n",
            "step 0100 | loss 0.6464 | acc 0.7726\n",
            "step 0200 | loss 0.6347 | acc 0.7772\n",
            "step 0300 | loss 0.6296 | acc 0.7796\n",
            "train  | loss 0.6250 acc 0.7809 time 236.3s\n",
            "valid  | loss 0.6049 acc 0.7879 macro_f1 0.7870\n",
            "\n",
            "==> alexnet-cifar10 | epoch 12/20\n",
            "step 0100 | loss 0.5986 | acc 0.7883\n",
            "step 0200 | loss 0.5928 | acc 0.7923\n",
            "step 0300 | loss 0.5889 | acc 0.7934\n",
            "train  | loss 0.5908 acc 0.7929 time 238.5s\n",
            "valid  | loss 0.5755 acc 0.7984 macro_f1 0.7994\n",
            "\n",
            "==> alexnet-cifar10 | epoch 13/20\n",
            "step 0100 | loss 0.5575 | acc 0.8045\n",
            "step 0200 | loss 0.5643 | acc 0.8018\n",
            "step 0300 | loss 0.5701 | acc 0.8004\n",
            "train  | loss 0.5713 acc 0.7994 time 238.0s\n",
            "valid  | loss 0.5809 acc 0.7975 macro_f1 0.7947\n",
            "\n",
            "==> alexnet-cifar10 | epoch 14/20\n",
            "step 0100 | loss 0.5467 | acc 0.8091\n",
            "step 0200 | loss 0.5505 | acc 0.8065\n",
            "step 0300 | loss 0.5479 | acc 0.8076\n",
            "train  | loss 0.5461 acc 0.8080 time 237.3s\n",
            "valid  | loss 0.5312 acc 0.8149 macro_f1 0.8151\n",
            "\n",
            "==> alexnet-cifar10 | epoch 15/20\n",
            "step 0100 | loss 0.5325 | acc 0.8124\n",
            "step 0200 | loss 0.5318 | acc 0.8115\n",
            "step 0300 | loss 0.5275 | acc 0.8137\n",
            "train  | loss 0.5259 acc 0.8142 time 235.4s\n",
            "valid  | loss 0.5432 acc 0.8158 macro_f1 0.8153\n",
            "\n",
            "==> alexnet-cifar10 | epoch 16/20\n",
            "step 0100 | loss 0.5094 | acc 0.8224\n",
            "step 0200 | loss 0.5019 | acc 0.8247\n",
            "step 0300 | loss 0.5062 | acc 0.8233\n",
            "train  | loss 0.5050 acc 0.8238 time 11182.0s\n",
            "valid  | loss 0.5121 acc 0.8235 macro_f1 0.8226\n",
            "\n",
            "==> alexnet-cifar10 | epoch 17/20\n",
            "step 0100 | loss 0.4946 | acc 0.8257\n",
            "step 0200 | loss 0.4936 | acc 0.8273\n",
            "step 0300 | loss 0.4907 | acc 0.8278\n",
            "train  | loss 0.4873 acc 0.8301 time 246.1s\n",
            "valid  | loss 0.5145 acc 0.8231 macro_f1 0.8223\n",
            "\n",
            "==> alexnet-cifar10 | epoch 18/20\n",
            "step 0100 | loss 0.4639 | acc 0.8382\n",
            "step 0200 | loss 0.4649 | acc 0.8391\n",
            "step 0300 | loss 0.4622 | acc 0.8400\n",
            "train  | loss 0.4641 acc 0.8393 time 230.2s\n",
            "valid  | loss 0.5311 acc 0.8143 macro_f1 0.8143\n",
            "\n",
            "==> alexnet-cifar10 | epoch 19/20\n",
            "step 0100 | loss 0.4500 | acc 0.8435\n",
            "step 0200 | loss 0.4469 | acc 0.8441\n",
            "step 0300 | loss 0.4454 | acc 0.8458\n",
            "train  | loss 0.4458 acc 0.8457 time 7822.1s\n",
            "valid  | loss 0.5079 acc 0.8285 macro_f1 0.8272\n",
            "\n",
            "==> alexnet-cifar10 | epoch 20/20\n",
            "step 0100 | loss 0.4376 | acc 0.8511\n",
            "step 0200 | loss 0.4442 | acc 0.8463\n",
            "step 0300 | loss 0.4388 | acc 0.8482\n",
            "train  | loss 0.4380 acc 0.8490 time 246.5s\n",
            "valid  | loss 0.4930 acc 0.8287 macro_f1 0.8306\n",
            "\n",
            ">> best val acc: 0.8287\n"
          ]
        }
      ],
      "source": [
        "# entrenar alexnet en cifar-10\n",
        "epochs_cifar10 = 20\n",
        "\n",
        "alexnet, history_cifar10 = fit(\n",
        "    model=alexnet,\n",
        "    train_loader=train_loader_cifar10,\n",
        "    val_loader=val_loader_cifar10,\n",
        "    criterion=criterion_cifar10,\n",
        "    optimizer=optimizer_cifar10,\n",
        "    scheduler=scheduler_cifar10,\n",
        "    device=device,\n",
        "    num_classes=10,\n",
        "    epochs=epochs_cifar10,\n",
        "    name=\"alexnet-cifar10\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ryosp6flo6",
      "metadata": {
        "id": "9ryosp6flo6"
      },
      "source": [
        "### Evaluación final del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92cs177axub",
      "metadata": {
        "id": "92cs177axub",
        "outputId": "d2094ff5-f10d-40d5-d0e7-734fc9096a18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== Resultados finales AlexNet en CIFAR-10 ==\n",
            "Test Loss: 0.4930\n",
            "Test Accuracy: 0.8287\n",
            "Test Macro-F1: 0.8306\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'val_loss': 0.49304004202160656,\n",
              " 'val_acc': 0.8287183544303798,\n",
              " 'val_macro_f1': 0.8305532336235046}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# evaluación final en test set\n",
        "final_stats_cifar10 = evaluate(alexnet, val_loader_cifar10, criterion_cifar10, device, num_classes=10)\n",
        "\n",
        "print(\"\\n== Resultados finales AlexNet en CIFAR-10 ==\")\n",
        "print(f\"Test Loss: {final_stats_cifar10['val_loss']:.4f}\")\n",
        "print(f\"Test Accuracy: {final_stats_cifar10['val_acc']:.4f}\")\n",
        "print(f\"Test Macro-F1: {final_stats_cifar10['val_macro_f1']:.4f}\")\n",
        "\n",
        "final_stats_cifar10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Métrica de desempeño:\n",
        "\n",
        "Las métricas utilizadas para evaluar el rendimiento del modelo son la precisión (accuracy), la matriz de confusión y el Macro-F1 Score. La precisión mide la proporción de predicciones correctas sobre el total de ejemplos, siendo una métrica básica pero importante para evaluar el rendimiento global del modelo. La matriz de confusión permite analizar el desempeño del modelo en términos de verdaderos positivos, falsos positivos, verdaderos negativos y falsos negativos, proporcionando una visión detallada de cómo el modelo clasifica cada clase. Finalmente, el Macro-F1 Score es crucial en este contexto porque toma en cuenta el balance entre precisión y recall para cada clase y luego calcula un promedio, lo que lo hace especialmente útil cuando se desea una evaluación equilibrada de todas las clases, incluso en conjuntos de datos con un número equilibrado de clases, como es el caso de CIFAR-10. Estas métricas, en conjunto, permiten una evaluación más completa del modelo, ayudando a identificar tanto la exactitud general como el rendimiento por clase."
      ],
      "metadata": {
        "id": "qYYNEex9aOQ8"
      },
      "id": "qYYNEex9aOQ8"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7KKrZn-_a01P"
      },
      "id": "7KKrZn-_a01P",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}