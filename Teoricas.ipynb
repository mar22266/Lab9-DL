{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89945eb6",
   "metadata": {},
   "source": [
    "## Integrantes \n",
    "- Andre Marroquin\n",
    "- Joaquin Puente\n",
    "- Sergio Orellana\n",
    "- Nelson Garcia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba41556",
   "metadata": {},
   "source": [
    "1) **¿En qué casos son útiles estas arquitecturas?**\n",
    "\n",
    "a) **GoogleNet (Inception)**\n",
    "\n",
    "Cuándo la usaría:\n",
    "\n",
    "- Cuando necesito capturar características a múltiples escalas (texturas finas y patrones grandes) en la misma capa.\n",
    "- Si busco buena precisión con presupuesto de cómputo moderado, por ejemplo en clasificación en la nube con recursos limitados.\n",
    "- Para reducir parámetros frente a CNNs clásicas profundas (gracias a 1×1 conv y “global average pooling”).\n",
    "\n",
    "**Por qué:** combina en paralelo conv 1×1, 3×3, 5×5 y pooling dentro de un módulo Inception, y concatena sus salidas; además usa clasificadores auxiliares durante el entrenamiento para estabilizar gradientes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00501c78",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![Google](GoogleNet.png)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6982ca",
   "metadata": {},
   "source": [
    "b) **DenseNet (Densely Connected Convolutional Networks)**\n",
    "\n",
    "Cuándo la usaría:\n",
    "\n",
    "- Cuando quiero máximo reuso de características y mejor flujo de gradiente (evitar desvanecimiento), útil en datasets medianos/pequeños.\n",
    "- Si necesito modelos relativamente compactos (sorprendentemente eficientes en parámetros para su profundidad).\n",
    "- En tareas donde ayuda combinar rasgos de bajo y alto nivel (ej. clasificación, segmentación).\n",
    "\n",
    "**Por qué:** cada capa recibe como entrada el concat de todas las salidas previas en el bloque denso; las transition layers controlan el crecimiento con 1×1 conv y pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b9f38a",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![DenseNet](DenseNet.png)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be28d7a",
   "metadata": {},
   "source": [
    "c) **MobileNet**\n",
    "\n",
    "Cuándo la usaría:\n",
    "\n",
    "- Para inferencia en dispositivos móviles/embebidos (apps on-device, IoT, robótica con tiempo real).\n",
    "- Cuando el requisito clave es baja latencia y bajo consumo con una caída mínima de precisión.\n",
    "- Para despliegues a gran escala donde el costo por consulta importa.\n",
    "\n",
    "**Por qué:** usa convoluciones separables en profundidad (depthwise + pointwise 1×1) y, en V2/V3, bottlenecks invertidos y ReLU6, logrando grandes ahorros en cómputo y parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02571350",
   "metadata": {},
   "source": [
    "![MobileNet](MobileNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b92843",
   "metadata": {},
   "source": [
    "d) **EfficientNet**\n",
    "\n",
    "Cuándo la usaría:\n",
    "\n",
    "- Cuando quiero mejor precisión-eficiencia y además escalar el modelo (pequeño → grande) de forma sistemática.\n",
    "- En sistemas donde puedo elegir entre variantes B0–B7 según mi presupuesto de FLOPs/memoria.\n",
    "- Para competir con SOTA en clasificación manteniendo un buen costo.\n",
    "\n",
    "**Por qué:** introduce el compound scaling (escala coordinadamente profundidad, ancho y resolución) y usa bloques MBConv con Squeeze-and-Excitation (inspirados en MobileNetV2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f362523b",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![EfficientNet](EfficientNet.png)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6336b",
   "metadata": {},
   "source": [
    "2) **¿Cómo puedo usar Transformers para image recognition?**\n",
    "\n",
    "Así lo haría con un Vision Transformer (ViT):\n",
    "\n",
    "1. Particionar la imagen en parches (p.ej., 16×16), aplanarlos y proyectarlos linealmente para obtener tokens; añado embeddings posicionales.\n",
    "2. Paso la secuencia por un encoder Transformer (múltiples capas de auto-atención multi-cabeza + MLP + normalizaciones).\n",
    "3. Prependo un token [CLS] (o equivalente) y su representación final alimenta una capa densa de clasificación.\n",
    "4. Usar pre-entrenamiento grande y fine-tuning en mi dataset; o variantes híbridas que mezclan CNNs y Transformers cuando quiero inductive bias local con contexto global.\n",
    "\n",
    "Cuándo lo usaría:\n",
    "\n",
    "- Cuando necesito contexto global explícito y flexibilidad para múltiples tareas (clasificación, detección, segmentación) sin depender de convoluciones.\n",
    "- Si dispongo de muchos datos (o técnicas de data-efficient training) y busco escalabilidad del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067bb82",
   "metadata": {},
   "source": [
    "# **Referencias**\n",
    "\n",
    "- GeeksforGeeks. (2025a, June 30). Depth wise Separable Convolutional Neural Networks. GeeksforGeeks. https://www.geeksforgeeks.org/machine-learning/depth-wise-separable-convolutional-neural-networks/ \n",
    "- GeeksforGeeks. (2025b, July 15). Understanding GoogLeNet Model CNN Architecture. GeeksforGeeks. https://www.geeksforgeeks.org/machine-learning/understanding-googlenet-model-cnn-architecture/ \n",
    "- GeeksforGeeks. (2025c, July 23). DenseNet explained. GeeksforGeeks. https://www.geeksforgeeks.org/computer-vision/densenet-explained/ \n",
    "- GeeksforGeeks. (2025d, July 23). EfficientNet Architecture. GeeksforGeeks. https://www.geeksforgeeks.org/computer-vision/efficientnet-architecture/ \n",
    "- GeeksforGeeks. (2025e, July 23). Mobilenet V2 Architecture in Computer Vision. GeeksforGeeks. https://www.geeksforgeeks.org/computer-vision/mobilenet-v2-architecture-in-computer-vision/ \n",
    "- GeeksforGeeks. (2025f, July 23). Vision Transformer in Computer vision. GeeksforGeeks. https://www.geeksforgeeks.org/computer-vision/vision-transformer-in-computer-vision/ \n",
    "- GeeksforGeeks. (2025g, July 23). Vision Transformers (VIT) in image recognition. GeeksforGeeks. https://www.geeksforgeeks.org/computer-vision/vision-transformers-vit-in-image-recognition/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
